{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e5c60ed-759b-4221-93e1-716b73e04490",
   "metadata": {},
   "source": [
    "# Text classification\n",
    "\n",
    "Example of a many-to-one problem using recurrent neural network. Following this [tutorial](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html).\n",
    "\n",
    "Let's try just like that.\n",
    "\n",
    "Required tasks:\n",
    "\n",
    "- load data\n",
    "- vectorize using one-hot encoding\n",
    "- write network class\n",
    "- setup training\n",
    "- train\n",
    "\n",
    "Next: \n",
    "- Implement a test set and test function\n",
    "- Done (I think): change loss to something that takes into account multi-label (and update my list to take that into account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d9d0f52-75c2-4900-9efc-17bf20809930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import pandas\n",
    "import glob\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import matplotlib.pyplot as plt\n",
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "def unicode_to_ascii(s, vocabulary):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in vocabulary\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe64b9a3-9528-44ea-b7a9-85dffa0a3dcd",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "## Create dictionaries that map language->name and name->language\n",
    "\n",
    "Certain files contain a lot of non-unique names. For example the \"Arabic.txt\" file contains 2000 lines but only 108 unique names. Here, we keep only the unique names. Note that the same name can appear in several languages. First, that means we can never achieve 100% accuracy. Second, we cannot store the examples as a name to language dictionary. Instead I'll use to lists: one for names and one for language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87fca392-adfc-47d2-bfc9-23584fe71b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = glob.glob(\"./Data/names/names/*\")\n",
    "languages = [re.findall(r'./Data/names/names/(.*)\\.txt',t)[0] for t in file_list]\n",
    "lang2name = {}\n",
    "name2lang = {}\n",
    "# vocabulary = string.ascii_letters + \" .,;-'\"\n",
    "vocabulary = string.ascii_lowercase + \" .,;'\"\n",
    "\n",
    "max_seq_length = 0\n",
    "name_list = []\n",
    "lang_list = []\n",
    "for lang, filename in zip(languages,file_list):\n",
    "    with open(filename) as file:\n",
    "        names = [l[:-1] for l in file.readlines()]\n",
    "        names = [unicode_to_ascii(n.lower(), vocabulary) for n in names]\n",
    "        names = list(set(names))\n",
    "        lang2name[lang] = names\n",
    "        name_list += names\n",
    "        lang_list += [lang]*len(names)\n",
    "        for n in names:\n",
    "            if not(n in name2lang):\n",
    "                name2lang[n] = []\n",
    "            name2lang[n] += [lang]\n",
    "            max_seq_length = max(max_seq_length,len(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6690f183-0752-4a54-b3d1-d8ed4b206152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Czech 502 0.00199203187250996\n",
      "German 690 0.0014492753623188406\n",
      "Arabic 108 0.009259259259259259\n",
      "Japanese 990 0.00101010101010101\n",
      "Chinese 246 0.0040650406504065045\n",
      "Vietnamese 70 0.014285714285714285\n",
      "Russian 9341 0.00010705491917353603\n",
      "French 273 0.003663003663003663\n",
      "Irish 226 0.004424778761061947\n",
      "English 3668 0.0002726281352235551\n",
      "Spanish 293 0.0034129692832764505\n",
      "Greek 193 0.0051813471502590676\n",
      "Italian 701 0.0014265335235378032\n",
      "Portuguese 74 0.013513513513513514\n",
      "Scottish 100 0.01\n",
      "Dutch 286 0.0034965034965034965\n",
      "Korean 94 0.010638297872340425\n",
      "Polish 138 0.007246376811594203\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Czech': 0.9999999999999992,\n",
       " 'German': 1.0000000000000053,\n",
       " 'Arabic': 1.0000000000000016,\n",
       " 'Japanese': 1.000000000000006,\n",
       " 'Chinese': 0.999999999999997,\n",
       " 'Vietnamese': 0.9999999999999983,\n",
       " 'Russian': 0.9999999999998247,\n",
       " 'French': 0.999999999999997,\n",
       " 'Irish': 1.0000000000000013,\n",
       " 'English': 0.9999999999999883,\n",
       " 'Spanish': 1.0000000000000069,\n",
       " 'Greek': 1.0000000000000024,\n",
       " 'Italian': 1.0000000000000053,\n",
       " 'Portuguese': 0.9999999999999986,\n",
       " 'Scottish': 1.0000000000000007,\n",
       " 'Dutch': 1.0000000000000062,\n",
       " 'Korean': 0.9999999999999983,\n",
       " 'Polish': 1.0000000000000013}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len()for key in lang2name.keys()\n",
    "total_names = 0\n",
    "lang2weights = {}\n",
    "for k, v in lang2name.items():\n",
    "    print(k, len(v),  1.0/(len(v)))\n",
    "    total_names += len(v)\n",
    "    lang2weights[k] = 1.0/len(v)\n",
    "\n",
    "# Check that every class is weighted equally\n",
    "total_weights = {}\n",
    "for k, v in lang2weights.items(): # init\n",
    "    total_weights[k] = 0\n",
    "\n",
    "count = 0    \n",
    "for name, lang in zip(name_list, lang_list):\n",
    "    total_weights[lang] += lang2weights[lang]\n",
    "    count += 1\n",
    "total_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfffe2aa-0c49-4a50-a918-b28e5fb98586",
   "metadata": {},
   "source": [
    "# Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c942f37-82a5-4cb2-bdac-6bd4eed9690d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, seq2cat, vocabulary, categories, max_seq_length):\n",
    "        # seq2cat (dict) mapping input sequence and output category\n",
    "        super(MyDataset,self).__init__()\n",
    "#         self.input_sequences = seq_list#list(seq2cat.keys())\n",
    "        self.input_sequences = list(seq2cat.keys())\n",
    "        self.output_category =  list(seq2cat.values())\n",
    "        self.categories = categories\n",
    "        self.vocabulary = vocabulary\n",
    "        self.max_seq_length = max_seq_length\n",
    "        \n",
    "        self.vocab_length = len(self.vocabulary)\n",
    "        self.vocab2vec = {}\n",
    "        for i, vocab in enumerate(self.vocabulary):\n",
    "            self.vocab2vec[vocab] = torch.zeros((1,self.vocab_length),dtype=torch.float32)\n",
    "            self.vocab2vec[vocab][0,i] = 1\n",
    "            \n",
    "        num_categories = len(categories)\n",
    "        self.cat2vec = {}\n",
    "        for i, cat in enumerate(categories):\n",
    "            self.cat2vec[cat] = torch.tensor(i,dtype=torch.long)\n",
    "            \n",
    "            \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_sequences)\n",
    "    \n",
    "    def _get_seq_vec(self,sequence):\n",
    "        # Pad \n",
    "        out = torch.zeros((self.max_seq_length,self.vocab_length))\n",
    "        for i, vec in enumerate(sequence.lower()):\n",
    "            out[i,:] = self.vocab2vec[vec]\n",
    "        return out\n",
    "#         Don't pad\n",
    "#         return torch.cat([self.vocab2vec[vec] for vec in sequence.lower()],axis=0)\n",
    "#         return torch.cat([self.vocab2vec[vec] for vec in sequence],axis=0)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "#         print(self.input_sequences[index])\n",
    "        input_sequence = self._get_seq_vec(self.input_sequences[index])\n",
    "    \n",
    "        output_category = 0.0*torch.ones(len(self.categories))\n",
    "        cat = self.output_category[index]\n",
    "#         print(cat)\n",
    "        for i in range(len(cat)):\n",
    "            output_category[self.cat2vec[cat[i]]] = 1.0\n",
    "\n",
    "#         print(input_sequence)\n",
    "#         print(output_category.shape)\n",
    "        \n",
    "        return (input_sequence, output_category)\n",
    "    \n",
    "# Test    \n",
    "dataset = MyDataset(name2lang, vocabulary, languages, max_seq_length)\n",
    "dataloader = torch.utils.data.DataLoader(dataset,batch_size=32, shuffle=True)\n",
    "inp, cat = next(iter(dataloader))\n",
    "inp[0].shape\n",
    "cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d36283-5583-4f92-96fc-804627d84e05",
   "metadata": {},
   "source": [
    "# Define neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1405ad14-6aa9-487a-8c07-72f8d81fa83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRNN(nn.Module):\n",
    "    def __init__(self,vocab_length, num_categories,num_layers=1,hidden_size=128, dropout=0.0):\n",
    "        super(MyRNN,self).__init__()\n",
    "        self.rnn = nn.RNN(input_size=vocab_length,\n",
    "                     hidden_size=hidden_size,#num_categories,\n",
    "                     num_layers=num_layers, batch_first=True, nonlinearity='relu',dropout=dropout)\n",
    "        self.linear = nn.Linear(hidden_size,num_categories)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "#         self.rnn = nn.LSTM(input_size=vocab_length,\n",
    "#                      hidden_size=num_categories,\n",
    "#                      num_layers=1, batch_first=True)\n",
    "#         self.linear = nn.Linear()\n",
    "    def forward(self,X):\n",
    "        # Pack padding to let the RNN know where to stop the sequence\n",
    "        seq_len = X.sum(dim=2).sum(dim=1).int()\n",
    "        X = torch.nn.utils.rnn.pack_padded_sequence(X,seq_len,batch_first=True, enforce_sorted=False)\n",
    "        out, H = self.rnn(X)\n",
    "        out, out_len = torch.nn.utils.rnn.pad_packed_sequence(out, batch_first=True)        \n",
    "        out = torch.cat([out[sample,out_len[sample]-1,:].view(1,1,-1) for sample in range(out.shape[0])],dim=0)\n",
    "        out = self.linear(self.relu(out))\n",
    "\n",
    "        return out.view(out.shape[0],out.shape[2])\n",
    "    \n",
    "# # Test  \n",
    "# num_categories = len(languages)\n",
    "# vocab_length = len(vocabulary)\n",
    "# model = MyRNN(vocab_length, num_categories,1)\n",
    "# dataset = MyDataset(name2lang, vocabulary, languages, max_seq_length)\n",
    "# dataloader = torch.utils.data.DataLoader(dataset,batch_size=1, shuffle=True)\n",
    "# inp = next(iter(dataloader))[0]\n",
    "# out = model(inp)\n",
    "# out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2a0fd2-7a4a-43b3-a3b8-ab186d7252bf",
   "metadata": {},
   "source": [
    "# Initialize training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "673dfb2b-4e61-4278-959b-61c8315c00e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_categories = len(languages)\n",
    "vocab_length = len(vocabulary)\n",
    "model = MyRNN(vocab_length, num_categories,3, hidden_size=128, dropout=.3)\n",
    "# model = HisRNN(vocab_length, n_hidden, num_categories)\n",
    "# loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
    "loss_fn = nn.MultiLabelSoftMarginLoss(weight=torch.tensor([lang2weights[l] for l in languages]))\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=5e-3)\n",
    "dataset = MyDataset(name2lang, vocabulary, languages, max_seq_length)\n",
    "dataloader = torch.utils.data.DataLoader(dataset,batch_size=64, shuffle=True)\n",
    "inp, cat = next(iter(dataloader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decc5d59-bc70-4109-8620-c27daa2af71b",
   "metadata": {},
   "source": [
    "# Train\n",
    "\n",
    "The dataset is imbalanced (9342 Russian examples, only 74 Portuguese). If we just run train over all the examples in the dataset the model just learns to predict everything as the one or two classes with the most examples (Russian and English). To overcome this difficulty we can choose between several strategy:\n",
    "\n",
    "- option 1: don't use a dataloader but iterate and choose one example per class at each iteration, or similarly, randomly sample a class and a sample from that class. That ensures that as many samples from each class is seen. The inconvenient is that samples from small classes are seen many times while samples for large classes are barely seen. This method is used [here](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html) (see their randomTrainingExample function).\n",
    "- option 2: subsampling, i.e. create a new dataset at each epoch which contains as many examples from each class (randomly chosen)\n",
    "- option 3: use a dataloader (goes through every examples once per epoch) and weight the loss function according to the class, where the weight is $w=1/n_{class}$, where $n_{class}$ is the number of example in that class. I'll use this option here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b3f80b0c-35ba-4781-a0cd-844535dd95ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 000: loss=3.12e-04, accuracy=4.80e-01\n",
      "epoch 001: loss=2.45e-04, accuracy=4.07e-01\n",
      "epoch 002: loss=2.22e-04, accuracy=4.49e-01\n",
      "epoch 003: loss=2.06e-04, accuracy=5.11e-01\n",
      "epoch 004: loss=1.97e-04, accuracy=5.41e-01\n",
      "epoch 005: loss=1.88e-04, accuracy=5.64e-01\n",
      "epoch 006: loss=1.84e-04, accuracy=5.85e-01\n",
      "epoch 007: loss=1.81e-04, accuracy=6.04e-01\n",
      "epoch 008: loss=1.76e-04, accuracy=6.02e-01\n",
      "epoch 009: loss=1.72e-04, accuracy=6.12e-01\n",
      "epoch 010: loss=1.71e-04, accuracy=6.18e-01\n",
      "epoch 011: loss=1.67e-04, accuracy=6.16e-01\n",
      "epoch 012: loss=1.67e-04, accuracy=6.19e-01\n",
      "epoch 013: loss=1.64e-04, accuracy=6.23e-01\n",
      "epoch 014: loss=1.61e-04, accuracy=6.33e-01\n",
      "epoch 015: loss=1.62e-04, accuracy=6.24e-01\n",
      "epoch 016: loss=1.59e-04, accuracy=6.26e-01\n",
      "epoch 017: loss=1.62e-04, accuracy=6.22e-01\n",
      "epoch 018: loss=1.57e-04, accuracy=6.32e-01\n",
      "epoch 019: loss=1.58e-04, accuracy=6.31e-01\n",
      "epoch 020: loss=1.53e-04, accuracy=6.37e-01\n",
      "epoch 021: loss=1.54e-04, accuracy=6.35e-01\n",
      "epoch 022: loss=1.53e-04, accuracy=6.34e-01\n",
      "epoch 023: loss=1.52e-04, accuracy=6.31e-01\n",
      "epoch 024: loss=1.54e-04, accuracy=6.30e-01\n",
      "epoch 025: loss=1.55e-04, accuracy=6.31e-01\n",
      "epoch 026: loss=1.51e-04, accuracy=6.36e-01\n",
      "epoch 027: loss=1.52e-04, accuracy=6.38e-01\n",
      "epoch 028: loss=1.54e-04, accuracy=6.32e-01\n",
      "epoch 029: loss=1.51e-04, accuracy=6.33e-01\n",
      "epoch 030: loss=2.20e-04, accuracy=5.76e-01\n",
      "epoch 031: loss=1.69e-04, accuracy=5.85e-01\n",
      "epoch 032: loss=1.59e-04, accuracy=6.11e-01\n",
      "epoch 033: loss=1.59e-04, accuracy=6.12e-01\n",
      "epoch 034: loss=1.56e-04, accuracy=6.21e-01\n",
      "epoch 035: loss=1.51e-04, accuracy=6.25e-01\n",
      "epoch 036: loss=1.51e-04, accuracy=6.22e-01\n",
      "epoch 037: loss=1.51e-04, accuracy=6.28e-01\n",
      "epoch 038: loss=1.50e-04, accuracy=6.31e-01\n",
      "epoch 039: loss=1.49e-04, accuracy=6.41e-01\n",
      "epoch 040: loss=1.50e-04, accuracy=6.43e-01\n",
      "epoch 041: loss=1.48e-04, accuracy=6.31e-01\n",
      "epoch 042: loss=1.46e-04, accuracy=6.45e-01\n",
      "epoch 043: loss=1.46e-04, accuracy=6.40e-01\n",
      "epoch 044: loss=1.49e-04, accuracy=6.39e-01\n",
      "epoch 045: loss=1.45e-04, accuracy=6.45e-01\n",
      "epoch 046: loss=1.56e-04, accuracy=6.33e-01\n",
      "epoch 047: loss=2.16e-04, accuracy=5.99e-01\n",
      "epoch 048: loss=1.72e-04, accuracy=6.00e-01\n",
      "epoch 049: loss=1.65e-04, accuracy=6.19e-01\n",
      "epoch 050: loss=1.57e-04, accuracy=6.34e-01\n",
      "epoch 051: loss=1.55e-04, accuracy=6.38e-01\n",
      "epoch 052: loss=1.59e-04, accuracy=6.29e-01\n",
      "epoch 053: loss=1.52e-04, accuracy=6.35e-01\n",
      "epoch 054: loss=1.56e-04, accuracy=6.29e-01\n",
      "epoch 055: loss=1.52e-04, accuracy=6.28e-01\n",
      "epoch 056: loss=1.52e-04, accuracy=6.35e-01\n",
      "epoch 057: loss=1.54e-04, accuracy=6.34e-01\n",
      "epoch 058: loss=1.55e-04, accuracy=6.32e-01\n",
      "epoch 059: loss=1.54e-04, accuracy=6.34e-01\n",
      "epoch 060: loss=1.53e-04, accuracy=6.29e-01\n",
      "epoch 061: loss=1.50e-04, accuracy=6.40e-01\n",
      "epoch 062: loss=1.52e-04, accuracy=6.34e-01\n",
      "epoch 063: loss=1.54e-04, accuracy=6.29e-01\n",
      "epoch 064: loss=1.52e-04, accuracy=6.35e-01\n",
      "epoch 065: loss=1.48e-04, accuracy=6.39e-01\n",
      "epoch 066: loss=1.49e-04, accuracy=6.35e-01\n",
      "epoch 067: loss=1.47e-04, accuracy=6.36e-01\n",
      "epoch 068: loss=1.46e-04, accuracy=6.42e-01\n",
      "epoch 069: loss=1.49e-04, accuracy=6.39e-01\n",
      "epoch 070: loss=1.50e-04, accuracy=6.33e-01\n",
      "epoch 071: loss=1.46e-04, accuracy=6.37e-01\n",
      "epoch 072: loss=1.54e-04, accuracy=6.27e-01\n",
      "epoch 073: loss=1.61e-04, accuracy=6.21e-01\n",
      "epoch 074: loss=1.51e-04, accuracy=6.31e-01\n",
      "epoch 075: loss=1.49e-04, accuracy=6.39e-01\n",
      "epoch 076: loss=1.55e-04, accuracy=6.23e-01\n",
      "epoch 077: loss=1.57e-04, accuracy=6.29e-01\n",
      "epoch 078: loss=1.50e-04, accuracy=6.33e-01\n",
      "epoch 079: loss=1.54e-04, accuracy=6.22e-01\n",
      "epoch 080: loss=1.52e-04, accuracy=6.35e-01\n",
      "epoch 081: loss=1.52e-04, accuracy=6.39e-01\n",
      "epoch 082: loss=1.54e-04, accuracy=6.27e-01\n",
      "epoch 083: loss=1.53e-04, accuracy=6.26e-01\n",
      "epoch 084: loss=1.48e-04, accuracy=6.35e-01\n",
      "epoch 085: loss=1.50e-04, accuracy=6.37e-01\n",
      "epoch 086: loss=1.51e-04, accuracy=6.29e-01\n",
      "epoch 087: loss=1.53e-04, accuracy=6.30e-01\n",
      "epoch 088: loss=1.64e-04, accuracy=6.13e-01\n",
      "epoch 089: loss=1.57e-04, accuracy=6.30e-01\n",
      "epoch 090: loss=1.57e-04, accuracy=6.19e-01\n",
      "epoch 091: loss=1.51e-04, accuracy=6.26e-01\n",
      "epoch 092: loss=1.52e-04, accuracy=6.14e-01\n",
      "epoch 093: loss=1.54e-04, accuracy=6.14e-01\n",
      "epoch 094: loss=1.49e-04, accuracy=6.23e-01\n",
      "epoch 095: loss=1.49e-04, accuracy=6.27e-01\n",
      "epoch 096: loss=1.48e-04, accuracy=6.32e-01\n",
      "epoch 097: loss=1.47e-04, accuracy=6.40e-01\n",
      "epoch 098: loss=1.45e-04, accuracy=6.42e-01\n",
      "epoch 099: loss=1.53e-04, accuracy=6.19e-01\n"
     ]
    }
   ],
   "source": [
    "# optimizer = torch.optim.Adam(model.parameters(),lr=5e-4)\n",
    "n_iter = len(dataloader)\n",
    "for epoch in range(100):\n",
    "    n_data = len(dataloader)\n",
    "    loss_total = 0\n",
    "    accuracy = 0\n",
    "    for i, (inp, ground_truth) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = model(inp)\n",
    "        loss = loss_fn(out, ground_truth)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            loss_total += loss\n",
    "            pred = torch.sigmoid(out)\n",
    "            thr = 0.5\n",
    "            num_gt = (ground_truth).sum()\n",
    "            accuracy += (pred[ground_truth == 1.0]>thr).sum()/num_gt\n",
    "\n",
    "    loss_total /= n_iter\n",
    "    accuracy /= n_iter\n",
    "    \n",
    "    print(f\"epoch {epoch:03d}: loss={loss_total:.2e}, accuracy={accuracy:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85988b00-47a9-47da-b2a8-6d692ee90fe1",
   "metadata": {},
   "source": [
    "# Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "aff78345-e5af-4798-a3b5-3dccd42e26e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x12faba880>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "conf_mat = np.zeros((len(languages),len(languages)))\n",
    "for i, (inp, ground_truth) in enumerate(dataloader):\n",
    "        out = model(inp)\n",
    "        with torch.no_grad():\n",
    "            pred = torch.sigmoid(out)\n",
    "            thr = 0.5\n",
    "            num_gt = (ground_truth).sum()\n",
    "            for i,j in zip(np.argwhere(pred>0.5)[0,:], np.argwhere(pred>0.5)[1,:]):\n",
    "                gt = np.argwhere(ground_truth[i,:])[0] # this takes only into account the first ground_truth, so it's not ideal (gives incomplete answer for multilabels)\n",
    "                conf_mat[gt,j] += 1.0\n",
    "for i, l in enumerate(languages):\n",
    "    conf_mat[i,:] /= np.sum(conf_mat[i,:])#len(lang2name[l])\n",
    "    plt.imshow(conf_mat)      \n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "50d99264-d588-4d4b-865b-89cc3a370127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "09d6d8d5-50d8-4a81-a4bd-546f3532cb02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "         36, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52,\n",
       "         53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argwhere(ground_truth==1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "f53ba2cf-a177-40c8-9760-d725f61b69e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09b6f3e-074b-49c1-bf07-df8e6f5d1cdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
