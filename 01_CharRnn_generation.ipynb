{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a20a1c82-2194-4a84-bb78-4b4cd75ce973",
   "metadata": {},
   "source": [
    "# Generate strings of text using a character-level LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada38fdd-7d5a-4f95-9808-8b0130cbfa32",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3db6ca31-0605-48d1-a431-fd0d510b27bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11729abb0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import string\n",
    "import pandas as pd\n",
    "import re\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e2c92d-f18e-495d-a1b7-17dc051fd45f",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efdffd2-cb68-4a30-8a76-9a559158a7cc",
   "metadata": {},
   "source": [
    "### Load and pre-process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06c955dd-0938-4101-903b-ca2ae77a65f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I The Time Traveller (for so it will be convenient to speak of him) was expounding a recondite matter to us.\\n',\n",
       " 'His grey eyes shone and twinkled, and his usually pale face was flushed and animated.\\n',\n",
       " 'The fire burned brightly, and the soft radiance of the incandescent lights in the lilies of silver caught the bubbles that flashed and passed in our glasses.\\n',\n",
       " 'Our chairs, being his patents, embraced and caressed us rather than submitted to be sat upon, and there was that luxurious after-dinner atmosphere when thought roams gracefully free of the trammels of precision.\\n',\n",
       " 'And he put it to us in this way--marking the points with a lean forefinger--as we sat and lazily admired his earnestness over this new paradox (as we thought it) and his fecundity.\\n']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./Data/timemachine.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "n_head = 35\n",
    "t = [line.strip() for line in lines[n_head:] if len(line.strip())>0]\n",
    "joined_text = \" \".join(t)\n",
    "my_text = [line.strip() + \".\" + \"\\n\" for line in joined_text.split(\".\")]\n",
    "my_text[:5] # check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c6e0f29-acd6-40dd-b0b1-f4c9cd8fc3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(set(joined_text))\n",
    "# vocab.sort()\n",
    "vocab.append('\\n')\n",
    "vocab_dict = dict([[v, ind] for ind, v in enumerate(vocab)])\n",
    "# vocab_dict\n",
    "vocab_length = len(vocab)\n",
    "vocab_weight = torch.zeros(vocab_length)\n",
    "for char in joined_text:\n",
    "    vocab_weight[vocab_dict[char]] += 1.0\n",
    "vocab_weight[vocab_dict['\\n']] = len(my_text)\n",
    "vocab_weight = 1.0/vocab_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b32745ff-01fb-4f57-8863-4ff817700545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I The Time Traveller (for so it will be convenient to speak of him) was expounding a recondite matter to us.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 1.]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_one_hot_vector(input_string):\n",
    "    out_vector = torch.zeros(len(input_string),1,vocab_length)\n",
    "    for i,c in enumerate(input_string):\n",
    "\n",
    "        out_vector[i,0,vocab_dict[c]] = 1\n",
    "    return out_vector\n",
    "\n",
    "print(my_text[0])\n",
    "get_one_hot_vector(my_text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fee0751-8bcf-4344-8c7c-3a731d2f01d5",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcae3b6e-f8a5-4a6a-888d-76dff57cbf18",
   "metadata": {},
   "source": [
    "# Predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d96c3a9d-21ab-4d43-a4aa-90502c0a6087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init hidden and cell states\n",
    "def predict(seed):\n",
    "    with torch.no_grad():\n",
    "        this_input = get_one_hot_vector(seed)\n",
    "        sentence_length = 20\n",
    "        out_string = \"\"\n",
    "        # init_state\n",
    "        out, (hn, cn) = model(this_input)\n",
    "    #     for i in range(this_input.shape[0]):\n",
    "    #         input = this_input[i,:,:].view(1,1,-1)\n",
    "    #         out, hn_cn = model(input, (hn, cn))\n",
    "    #         out_string += vocab[int(input.view(-1).argmax())]\n",
    "        out = out[-1:,:,:]\n",
    "        for i in range(sentence_length):\n",
    "    #         hn = torch.randn(D*num_layers, N, H_cell)\n",
    "    #         cn = torch.randn(D*num_layers, N, H_cell)\n",
    "            out, (hn, cn) = model(out, (hn, cn))\n",
    "            out_string += vec2string(out)\n",
    "    print(\"< \" + out_string + \" >\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "309629b3-b451-4686-b0fb-8da1d5a9f81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec2string(input):\n",
    "    out_string = \"\"\n",
    "    for i in range(len(input)):\n",
    "        this_input = input[i,0,:].view(-1)\n",
    "        out_string += vocab[int(this_input.argmax())]\n",
    "    return out_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53a0182-71a4-4c49-81ff-210f1bd28dfd",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8dcd38a-a63b-4dda-818c-6617ce27d94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sentence_maker(nn.Module):\n",
    "    def __init__(self, H_in, H_cell, num_layers=1, dropout = .0):\n",
    "        super(sentence_maker, self).__init__()\n",
    "        H_out = H_in\n",
    "        self.D = 1\n",
    "        self.H_cell = H_cell\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(H_in, H_cell, \n",
    "                            num_layers = num_layers, \n",
    "                            dropout = dropout)\n",
    "        self.linear = nn.Linear(H_cell, H_out)\n",
    "        self.logsoftmax = nn.LogSoftmax(2)\n",
    "        \n",
    "    def forward(self, input, states=None):\n",
    "        if states==None:\n",
    "            out, states = self.lstm(input)\n",
    "        else:\n",
    "            out, states = self.lstm(input, states)\n",
    "        out = self.linear(out)\n",
    "        out = self.logsoftmax(out)\n",
    "        return out, states\n",
    "    def begin_state(self, batch_size=1):\n",
    "        return (torch.zeros((self.D * self.num_layers,\n",
    "                             batch_size, self.H_cell)),\n",
    "                torch.zeros((self.D * self.num_layers,\n",
    "                             batch_size, self.H_cell)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fe2b18f-78aa-48c3-bae8-c04c637d3cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "H_cell = 256 # hidden size\n",
    "N = 1 # batch size\n",
    "L = 8 # sequence length\n",
    "D = 1 # unidirectional\n",
    "num_layers = 2 # /!\\ not yet inputted to LSTM function\n",
    "H_in = vocab_length # input size\n",
    "\n",
    "H_out = H_cell # output size, H_cell by default or project size if project_size>0\n",
    "\n",
    "\n",
    "# lstm = nn.LSTM(H_in, H_cell, proj_size=proj_size)\n",
    "model = sentence_maker(H_in, H_cell,num_layers=num_layers, dropout=0.0)\n",
    "\n",
    "loss_func = nn.NLLLoss(weight=vocab_weight)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=1.0e-2)\n",
    "\n",
    "hn = torch.zeros(D*num_layers, N, H_cell)\n",
    "cn = torch.zeros(D*num_layers, N, H_cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fb0a8f-04c2-440d-b26c-3bc38a3f20ac",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4a05fc3-6fca-49ba-85ab-047639ee200c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #00: l=2.16e+02\n",
      "  in:      p://pgla\n",
      "  target:  ://pglaf\n",
      "  out:     \n",
      "\n",
      "\n",
      "s.\n",
      "s.\n",
      "Epoch #01: l=1.99e+02\n",
      "  in:      please v\n",
      "  target:  lease vi\n",
      "  out:     \n",
      "dddddwh\n",
      "Epoch #02: "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-66ecd0ed97ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi_sample\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi_sample\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mi_batch\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epoch = 100\n",
    "batch_size = 50\n",
    "n_batch = len(my_text)//batch_size\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "\n",
    "    print(f\"Epoch #{epoch:02d}: \", end=\"\")\n",
    "    loss = 0.0    \n",
    "    i_batch = 0\n",
    "    for i_sample in range(len(my_text)):\n",
    "    # for i_sample in range(3):    \n",
    "        hn.detach_()\n",
    "        cn.detach_()\n",
    "        sentence = my_text[i_sample]\n",
    "        this_L = min(len(sentence),L+1)-1\n",
    "        if this_L < L:\n",
    "            I=0\n",
    "        else:\n",
    "            I = np.random.randint(0,len(sentence)-this_L) \n",
    "            \n",
    "        # Get a random subsentence    \n",
    "        \n",
    "        this_input = get_one_hot_vector(sentence[I:I+this_L])\n",
    "        this_target = get_one_hot_vector(sentence[I+1:I+1+this_L])\n",
    "\n",
    "        # Forward/backward pass\n",
    "        \n",
    "        out, (hn, cn) = model(this_input)#, (hn, cn))\n",
    "        \n",
    "        loss += loss_func(out.view(this_L,H_in),this_target.argmax(dim=2).view(-1))\n",
    "        if i_sample>0 and i_sample%batch_size==0:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            i_batch+=1\n",
    "#             print(i_batch, n_batch)\n",
    "            if i_batch==n_batch:\n",
    "                print(f\"l={loss:.2e}\")\n",
    "                print(\"  in:     \", vec2string(this_input))\n",
    "                print(\"  target: \", vec2string(this_target))\n",
    "                print(\"  out:    \", vec2string(out))\n",
    "#                 predict()\n",
    "#             print(f\"{i_batch:03d}/{n_batch:03d} l={loss:.2e}\")\n",
    "            loss = 0.0\n",
    "\n",
    "print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a11e2b0-472b-4982-9525-2d9c392fcea9",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb3d6568-996e-4570-80f5-9ca24faf8691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< nnnnnnnnnnnnnnnnnnnn >\n"
     ]
    }
   ],
   "source": [
    "predict(\"I\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9889a69c-e17c-4698-bc13-2a843f1742fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-b5927d60fa7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "out[-1,:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d70f2ff3-8987-457a-93e2-76c51c99e45d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 82])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88131697-a6e1-47f0-ba29-e954118f1c74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " '!',\n",
       " '\"',\n",
       " '$',\n",
       " '%',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " '@',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " '[',\n",
       " ']',\n",
       " '_',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '\\n']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c41ea6-a44b-44e4-bbcb-82a78b502079",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
